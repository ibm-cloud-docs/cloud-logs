document_outline: |
  Information about the IBM Cloud Logs offering on IBM Cloud. IBM Cloud Logs is a scalable logging service that persists logs and provides users with capabilities for querying, tailing, and visualizing logs.
domain: IBM Cloud Logs
seed_examples:
  - context: |
      # About {{site.data.keyword.logs_full_notm}}
      {: #about-cl}

      {{site.data.keyword.logs_full}} provides observability services for {{site.data.keyword.cloud_notm}} so you can view, analyze, and alert on activity tracking events and logging activity. Logging data can be sent from orchestrated and nonorchestrated environments.
      {: shortdesc}

      As workloads generate an expanding amount of observability data, pressure is increasing on collection tools to process all the data. The data becomes expensive to manage and makes it harder to obtain actionable insights. It is harder to have fast, effective, and cost-efficient operational and performance management.

      {{site.data.keyword.logs_full_notm}} is designed to help users take control of their observability data and expedite insights to reduce application downtime.

      {{site.data.keyword.logs_full_notm}} supports integration with common workload environments on {{site.data.keyword.cloud_notm}} including {{site.data.keyword.containerlong_notm}} and {{site.data.keyword.openshiftlong_notm}}. Integration with non-orchestrated environments, such as Linux and Windows, is also supported.

      With {{site.data.keyword.logs_full_notm}}, you can send both log data and activity tracking events into the service, which gives you flexibility in how you handle your data. Log and event data can be sent to separate {{site.data.keyword.logs_full_notm}} instances or combined into a single instance to expand observability insights.

      {{site.data.keyword.logs_full_notm}} processes incoming data and applies machine learning algorithms, including log aggregation and anomaly detection. This processing helps you focus on the root cause of issues.
    questions_and_answers:
      - question: What is the primary function of {{site.data.keyword.logs_full_notm}} in the context of IBM Cloud?
        answer: | 
          {{site.data.keyword.logs_full_notm}} is an observability service on IBM Cloud designed to help users manage, analyze, and gain insights from activity tracking events and logging data generated by workloads. It supports integration with various environments, including containerized, non-containerized, and orchestrated platforms like Kubernetes and OpenShift.
      - question: How does {{site.data.keyword.logs_full_notm}} assist in handling the challenges of growing observability data?
        answer: |
           {{site.data.keyword.logs_full_notm}} processes incoming log data and activity tracking events, applying machine learning algorithms such as log aggregation and anomaly detection. This processing aids users in focusing on the root cause of issues by providing insights and patterns within the data.
      - question: What types of environments can {{site.data.keyword.logs_full_notm}} integrate with for collecting observability data?
        answer: | 
          {{site.data.keyword.logs_full_notm}} can receive both log data and activity tracking events. Users have the flexibility to send this data to separate instances or combine them into a single instance to enhance observability insights.
  - context: |
      # Enriching data
      {: #enriching-data}

      You can easily enrich your log data with {{site.data.keyword.logs_full_notm}}. You can automatically add fields to your JSON logs based on specific matches in your log data by using a pre-defined custom data source of your own. This way, you can enhance your log data with business, operations, or security information that is not available at run time.
      {: shortdesc}

      You can enrich your logs in two possible ways:

      * Select a log key to be used to look up a key value and enrich the logs automatically during ingestion. The logs are saved with the enriched fields. The advantages of this mode are:

        * Logs are automatically enriched.

        * The logs include the enrichment data, which can be consumed everywhere (for example, in any query and also by third-party products that read the logs from the bucket).

      * Use the DataPrime query [`enrich`](/docs/cloud-logs?topic=cloud-logs-dataprime-ref#enrich) to look up a value in a table and enrich the log dynamically for the query. The advantages of this mode are:

        * You can enrich old logs already ingested into {{site.data.keyword.logs_full_notm}}.

        * The enrichment does not increase the size of the stored logs, since the enrichment is done dynamically, and is only used for the query results.

      ## Data enrichment use cases
      {: #enrich-use-cases}

      Some example use cases where enrichment can be helpful are:

      ### Monitoring
      {: #enrich-monitoring}

      In this example, assume we have logs with a UUID representing a customer. However, no field exists in the log with the customer name.

      You can enrich the log by adding a field containing the customer name so you can visualize and search the logs base on the name. With custom enrichment, you create the enrichment by setting up a CSV file to map each UUID to a customer name.

      ### Security
      {: #enrich-security}

      In this example, the logs contain a field with a domain name that represents where an application is accessed. You want to create an alert that creates a notification if an attempt to access the application is made from an unauthorized domain.

      You can create a CSV file with a list of allowlisted domains so each log is enriched with a field (`domain_enriched`) with the word `allowed` for domains in the list. You can then create an alert for logs that do not contain this field (for example `NOT domain_enriched:allowed`).
    questions_and_answers:
      - question: How can you enrich log data with {{site.data.keyword.logs_full_notm}}?
        answer: | 
          You can enrich log data with {{site.data.keyword.logs_full_notm}} in two ways: by automatically adding fields during ingestion using a selected log key to look up a key value, or by using the DataPrime query `enrich` to dynamically enrich logs for a specific query based on a table lookup.
      - question: How does {{site.data.keyword.logs_full_notm}} help in identifying the root cause of issues?
        answer: |
          {{site.data.keyword.logs_full_notm}} processes incoming log data and activity tracking events, applying machine learning algorithms such as log aggregation and anomaly detection. This processing aids users in focusing on the root cause of issues by providing insights and patterns within the data.
      - question: What are the two methods for enriching logs in {{site.data.keyword.logs_full_notm}}? 
        answer: |
          There are two methods for enriching logs in {{site.data.keyword.logs_full_notm}}:

          * Automatically enrich logs during ingestion by selecting a log key to look up a key value in a pre-defined custom data source. This method saves enriched logs with additional fields, which can be consumed in queries and by third-party products.
          * Use the DataPrime query `enrich` function to look up a value in a table and dynamically enrich logs for a specific query. This method allows enrichment of old logs already ingested into {{site.data.keyword.logs_full_notm}} without increasing the size of stored logs, as enrichment is done dynamically and only for query results.
  - context: |
      # Using benchmark tags
      {: #benchmarks}

      Version benchmark tags can be used as a representation of any significant change or event that might impact your system. They can be received automatically from your CI/CD pipelines or inserted manually.
      {: shortdesc}

      Version benchmark tags are a way for you to understand your version status at a glance, integrate with your deployment pipeline, and get your latest build status. 


      ## Creating a benchmark tag
      {: #create_tag}


      {{site.data.content.launch-ui}}

      2. Click the **Dashboards** icon ![Dashboards icon](/icons/dashboards.svg "Dashboards") > **Version benchmarks**.

      3. Click **New Tag**.

      4. Enter a description for the tag.

      5. Enter the tag content including the timeframe associated with the tag and the applications and subsystems associate with the tag.

      ## Tag usage
      {: #tag_usage}

      Version benchmark tags can be used to track scenarios similar to the following:

      * View the change in triggered anomalies, alerts, error volume, newly introduced errors, and high severity logs ratio since the tag time. Compare this to the corresponding timeframe in the comparison tag.

      * View the number of errors since the tag in comparison to the period prior to the tag.

      * View the number of alerts since the tag, grouped by alert name and compared to the previous period.

      * View the errors that arrived in numbers greater than their normal ratio since the tag.

      * View the logs appearing on your system for the first time since the tag.
    questions_and_answers:
      - question: How do I create a benchmark tag?
        answer: |
          {{site.data.content.launch-ui}}
          2. Click the **Dashboards** icon ![Dashboards icon](/icons/dashboards.svg "Dashboards") > **Version benchmarks**.
          3. Click **New Tag**.
          4. Enter a description for the tag.
          5. Enter the tag content including the timeframe associated with the tag and the applications and subsystems associate with the tag.
      - question: Why should I use version benchmark tags? 
        answer: Version benchmark tags are a way for you to understand your version status at a glance, integrate with your deployment pipeline, and get your latest build status. 
      - question: Can you share examples of using version benchmark tags?
        answer: |
          Yes. Here's some example scenarios of where version benchmark tags would be useful:
          * View the change in triggered anomalies, alerts, error volume, newly introduced errors, and high severity logs ratio since the tag time. Compare this to the corresponding timeframe in the comparison tag.
          * View the number of errors since the tag in comparison to the period prior to the tag.
          * View the number of alerts since the tag, grouped by alert name and compared to the previous period.
          * View the errors that arrived in numbers greater than their normal ratio since the tag.
          * View the logs appearing on your system for the first time since the tag.
  - context: |
      # Viewing log data by time interval
      {: #query-data-time}

      The log data displayed by {{site.data.keyword.logs_full_notm}} can be filtered by a specific time interval.
      {: shortdesc}

      Limiting by time interval can be used in conjunction with [filtering](/docs/cloud-logs?topic=cloud-logs-query-data-filter), searching using [Lucene](/docs/cloud-logs?topic=cloud-logs-query-data-lucene) or [DataPrime](/docs/cloud-logs?topic=cloud-logs-query-data-dataprime).
      {: tip}


      {{site.data.content.query-ui}}

      ## Specifying a time interval
      {: #use-time}

      By default the **last 15 minutes** of data is displayed. You can change the time interval of log data displayed.

      1. Click the time interval selection. If unchanged, this will read **Last 15 minutes**. If changed, the selected interval will be displayed.

      2. Select the interval to be included:

        Quick
        :   Lets you quickly select a time interval. For example, all log data for **Today** or in the **Last 6 hours**. For **Quick** intervals, you can specify how often the query is automatically refreshed.

        Relative
        :   Lets you specify a time interval relative to the current time looking back the configured seconds, minutes, hours or days for the configured seconds, minutes, hours or days. For example, 6 days ago until 3 days ago.

        Custom
        :   Lets you specify a specific start and end date and time to be displayed.

        Tags
        :   Lets you filter logs by [tags](/docs/cloud-logs?topic=cloud-logs-benchmarks).
    questions_and_answers: 
      - question: Why should I limit my log data view by time interval?
        answer: Limiting by time interval can be used in conjunction with [filtering](/docs/cloud-logs?topic=cloud-logs-query-data-filter), searching using [Lucene](/docs/cloud-logs?topic=cloud-logs-query-data-lucene) or [DataPrime](/docs/cloud-logs?topic=cloud-logs-query-data-dataprime).
      - question: What's the default time interval?
        answer: The default time interval is 15 minutes.
      - question: How do I specify a time interval?
        answer: |
          1. Click the time interval selection. If unchanged, this will read **Last 15 minutes**. If changed, the selected interval will be displayed.
          2. Select the interval to be included. Options are Quick, Relative, Custom, and Tags.
  - context: |
      # Why is the **All Logs** option not available?
      {: #ts-no-all-logs}
      {: troubleshoot}
      {: support}

      In {{site.data.keyword.logs_full}}, when viewing logs, you cannot select the **All Logs** option.
      {: shortdesc}


      The **All Logs** option is disabled and you cannot select it. You can select only {{site.data.keyword.frequent-search}}.
      {: tsSymptoms}


      Some issues that can cause data that is received by {{site.data.keyword.logs_full_notm}} to be only available in {{site.data.keyword.frequent-search}} include:
      {: tsCauses}

      - No data bucket is configured for the {{site.data.keyword.logs_full_notm}} instance. Data is only maintained for the retention period for the {{site.data.keyword.logs_full_notm}} instance [service plan](/docs/cloud-logs?topic=cloud-logs-service_plans). After the retention period, the data is dropped.

        A data bucket is required for data to be configured to be sent by TCO policies to the {{site.data.keyword.monitoring}} and {{site.data.keyword.compliance}} pipelines. A data bucket is also required for data that is received by {{site.data.keyword.logs_full_notm}} to be automatically archived and accessible using **All Logs**.
      - Service to service authorization is not configured correctly between {{site.data.keyword.logs_full_notm}} and {{site.data.keyword.cos_full_notm}}.


      Resolve the issue based on the cause:
      {: tsResolve}

      - [Configure a data bucket](/docs/cloud-logs?topic=cloud-logs-configure-data-bucket) for your {{site.data.keyword.logs_full_notm}} instance, if one is not configured.
      - Check that your [service to service authorization](/docs/cloud-logs?topic=cloud-logs-iam-service-auth-cos&interface=ui) to the {{site.data.keyword.cos_full_notm}} bucket is correctly configured. Update the authorization if it is incorrect.
    questions_and_answers:
      - question: I can't select the All Logs option
        answer: |
          Some issues that can cause data that is received by {{site.data.keyword.logs_full_notm}} to be only available in {{site.data.keyword.frequent-search}} include:

          - No data bucket is configured for the {{site.data.keyword.logs_full_notm}} instance. Data is only maintained for the retention period for the {{site.data.keyword.logs_full_notm}} instance [service plan](/docs/cloud-logs?topic=cloud-logs-service_plans). After the retention period, the data is dropped.
          - Service to service authorization is not configured correctly between {{site.data.keyword.logs_full_notm}} and {{site.data.keyword.cos_full_notm}}.
      - question: How can I enable the All Logs option?
        answer: |
          To enable the All Logs option:
          - [Configure a data bucket](/docs/cloud-logs?topic=cloud-logs-configure-data-bucket) for your {{site.data.keyword.logs_full_notm}} instance, if one is not configured.
          - Check that your [service to service authorization](/docs/cloud-logs?topic=cloud-logs-iam-service-auth-cos&interface=ui) to the {{site.data.keyword.cos_full_notm}} bucket is correctly configured. Update the authorization if it is incorrect.
     
